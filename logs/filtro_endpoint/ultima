--------------logsPage.js--------------------------
import React, { useState } from 'react';

const LogsPage = () => {
  // Definimos todos los estados necesarios
  const [archivo, setArchivo] = useState(null);
  const [resultado, setResultado] = useState(null);
  const [cargando, setCargando] = useState(false);
  const [error, setError] = useState(null);

  const manejarSubida = async (e) => {
    e.preventDefault();
    if (!archivo) return;

    setCargando(true);
    setError(null);

    try {
      const formData = new FormData();
      formData.append('file', archivo);

      const respuesta = await fetch('http://localhost:8000/procesar-log/', {
        method: 'POST',
        mode: 'cors',
        headers: {
          'Accept': 'application/json',
        },
        body: formData
      });

      if (!respuesta.ok) {
        const errorText = await respuesta.text();
        throw new Error(`Error del servidor: ${respuesta.status} - ${errorText}`);
      }

      const datos = await respuesta.json();
      setResultado(datos);

    } catch (err) {
      setError(`Error al conectar con el servidor: ${err.message}`);
      console.error("Detalles del error:", err);
    } finally {
      setCargando(false);
    }
  };

  return (
    <div className="contenedor-logs">
      <h2>Analizador de Logs</h2>
      
      <form onSubmit={manejarSubida}>
        <input
          type="file"
          accept=".txt,.log"
          onChange={(e) => setArchivo(e.target.files[0])}
          disabled={cargando}
        />
        <button type="submit" disabled={!archivo || cargando}>
          {cargando ? 'Procesando...' : 'Analizar Logs'}
        </button>
      </form>

      {error && <div className="error">{error}</div>}

      {resultado && (
        <div className="resultado">
          <h3>Resultados:</h3>
          <pre>{JSON.stringify(resultado, null, 2)}</pre>
          <p>Logs procesados: {resultado.total_logs}</p>
          <p>Bytes analizados: {resultado.bytes_processed}</p>
        </div>
      )}
    </div>
  );
};

export default LogsPage;

--------------------------------------filtro endpoit----------------------------filtro_api.py

from fastapi import FastAPI, HTTPException, UploadFile, File
from fastapi.responses import JSONResponse
import os
import sys
import re
import tempfile
from collections import defaultdict
from insertar import Logger
from consumos.consulta_ia_openai import Consulta_ia_openai
from metodos_loprocesos import ProcesosLogger
from fastapi.middleware.cors import CORSMiddleware
import logging

# --- Configuraci√≥n id√©ntica a tu original ---
os.environ['NO_PROXY'] = 'recursoazureopenaimupi.openai.azure.com'
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..')))

# Configuraci√≥n para mantener tus prints en consola
logging.basicConfig(level=logging.INFO, format='%(message)s')
logger = logging.getLogger(__name__)
logger.addHandler(logging.StreamHandler())

app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000"],  # URL de tu frontend
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- Copia exacta de todas tus funciones originales ---
PRIORIDAD = ['ERROR', 'WARN', 'INFO', 'DEBUG', 'UNKNOWN']

CATEGORIAS = {
    'start_send': re.compile(r'inicia envio', re.IGNORECASE),
    'end_send': re.compile(r'fin envio', re.IGNORECASE),
    'ftp_error': re.compile(r'FTP.*ERROR', re.IGNORECASE),
    'general_error': re.compile(r'ERROR', re.IGNORECASE),
}

def es_inicio_log(linea: str) -> bool:
    return bool(re.match(r"\d{2}:\d{2}:\d{2},\d{3}", linea))

def extraer_componente(linea: str) -> str:
    match = re.search(r'\b(?:ERROR|WARN|INFO|DEBUG)\s+\[([^\]]+)\]', linea)
    return match.group(1).strip() if match else "desconocido"

def extraer_hilo(linea: str) -> str:
    match = re.search(r'\(([^)]+)\)', linea)
    return match.group(1).strip() if match else "main"

def extraer_nivel(linea: str) -> str:
    niveles = ['ERROR', 'WARN', 'INFO', 'DEBUG']
    for nivel in niveles:
        if f' {nivel} ' in linea:
            return nivel
    return 'UNKNOWN'

def categorizar_mensaje(texto: str) -> str:
    for categoria, patron in CATEGORIAS.items():
        if patron.search(texto):
            return categoria
    return 'otros'

def limitar_longitud(texto: str, max_len=30000):
    return texto if len(texto) <= max_len else texto[:max_len] + '...'

def prioridad_nivel(nivel):
    return PRIORIDAD.index(nivel) if nivel in PRIORIDAD else len(PRIORIDAD)

def contar_logs_procesados(file_path: str) -> int:
    with open(file_path, 'r', encoding='utf-8') as file:
        return sum(1 for line in file if line.startswith('# Bloque encontrado'))

def extraer_bloques_log(chunk: str, offset_linea: int = 0) -> list:
    bloques = []
    lineas = chunk.splitlines(keepends=True)
    bloque_actual = []
    en_bloque = False
    linea_inicio = None

    for i, linea in enumerate(lineas, start=offset_linea):
        if es_inicio_log(linea):
            if en_bloque:  
                bloques.append({
                    'linea_inicio': linea_inicio,
                    'contenido': "".join(bloque_actual)
                })
            bloque_actual = [linea]
            en_bloque = True
            linea_inicio = i
        elif en_bloque:
            if linea.startswith(("   ", "\t", "at ")):
                bloque_actual.append(linea)
            else:
                bloques.append({
                    'linea_inicio': linea_inicio,
                    'contenido': "".join(bloque_actual)
                })
                en_bloque = False
                bloque_actual = []

    if en_bloque and bloque_actual:
        bloques.append({
            'linea_inicio': linea_inicio,
            'contenido': "".join(bloque_actual)
        })
    return bloques

def procesar_bloque(bloque_actual, linea_inicio, reporte):
    mensaje_completo = "".join(bloque_actual).strip()
    nivel = extraer_nivel(mensaje_completo)
    categoria = categorizar_mensaje(mensaje_completo)
    componente = extraer_componente(mensaje_completo)
    hilo = extraer_hilo(mensaje_completo)

    mensaje_normalizado = re.sub(r'^\d{2}:\d{2}:\d{2},\d{3}\s*', '', mensaje_completo).strip()
    
    if "FTP MKDIR" in mensaje_normalizado:
        mensaje_normalizado = re.sub(r'FTP MKDIR.*?ERROR', 'FTP MKDIR [DIRECTORIO] ERROR', mensaje_normalizado)

    clave = (nivel, categoria, mensaje_normalizado)
    reporte[clave].update({
        'count': reporte[clave]['count'] + 1,
        'lineas': reporte[clave]['lineas'] + [linea_inicio],
        'nivel': nivel,
        'categoria': categoria,
        'componente': componente,
        'hilo': hilo,
        'mensaje': mensaje_completo,
        'mensaje_normalizado': mensaje_normalizado
    })

def insertar_logs_a_bd(reporte):
    total_insertados = 0
    consulta = Consulta_ia_openai()
    
    for (nivel, categoria, _), datos in reporte.items():
        if nivel not in ['WARN', 'ERROR']:
            continue
            
        try:
            if not Logger.existe_error_en_bd(datos['mensaje_normalizado'], nivel):
                respuesta_openai = None
                
                if nivel == 'ERROR':
                    mensaje_para_ia = limitar_longitud(datos['mensaje_normalizado'], max_len=2000)
                    respuesta_openai = consulta.interpretar_logs(mensaje_para_ia)
                    print(f"üîç Soluci√≥n OpenAI para error: {respuesta_openai[:100]}...")
                
                Logger.insertar_log(
                    idEmpresa=1,
                    operador=0,
                    mensaje=datos['mensaje_normalizado'],
                    nivel=nivel,
                    componente=datos['componente'],
                    hilo=datos['hilo'],
                    categoria=categoria,
                    estado='ACTIVO',
                    lineas=datos['lineas'],
                    ocurrencias=datos['count'],
                    respuestaOpenai=respuesta_openai
                )
                total_insertados += 1
                print(f"‚úÖ Log insertado: {nivel} - {datos['mensaje_normalizado'][:100]}...")
            else:
                print(f"‚ö†Ô∏è Log duplicado (no insertado): {nivel} - {datos['mensaje_normalizado'][:100]}...")
        except Exception as e:
            print(f"‚ùå Error insertando log: {str(e)}")
    
    return total_insertados

def generar_reporte_logs(bloques: list) -> dict:
    reporte = defaultdict(lambda: {
        'count': 0,
        'lineas': [],
        'nivel': '',
        'categoria': '',
        'componente': '',
        'hilo': '',
        'mensaje': '',
        'mensaje_normalizado': ''
    })

    for bloque in bloques:
        lineas_bloque = bloque['contenido'].split('\n')
        procesar_bloque(lineas_bloque, str(bloque['linea_inicio']), reporte)

    insertar_logs_a_bd(reporte)
    return reporte

@app.post("/procesar-log/")
async def procesar_log(file: UploadFile = File(...)):
    """Endpoint que replica exactamente tu flujo original"""
    temp_path = os.path.join(tempfile.gettempdir(), file.filename)
    
    try:
        # Guardar archivo subido
        with open(temp_path, "wb") as buffer:
            buffer.write(await file.read())

        # Validar archivo
        if not os.path.exists(temp_path):
            print(f"‚ùå Archivo no encontrado: {temp_path}")
            raise HTTPException(status_code=404, detail="Archivo no encontrado")

        # Flujo id√©ntico a tu script original
        bloque = ProcesosLogger.reservar_bloque(
            ruta_archivo=temp_path,
            idEmpresa=1,
            operador=0,
            bloque_size=1048576
        )
        
        if not bloque:
            print("‚ùå No hay bytes nuevos por procesar o error al reservar bloque.")
            raise HTTPException(status_code=400, detail="No hay bytes nuevos por procesar")

        print(f"üîç Iniciando procesamiento (bytes {bloque['byte_inicio']}-{bloque['byte_fin']})...")

        try:
            with open(temp_path, 'rb') as f:
                f.seek(bloque['byte_inicio'])
                chunk = f.read(bloque['byte_fin'] - bloque['byte_inicio'] + 1).decode('utf-8', errors='ignore')
            
            bloques_procesados = extraer_bloques_log(chunk, offset_linea=0)
            reporte = generar_reporte_logs(bloques_procesados)
            total_logs = sum(datos['count'] for datos in reporte.values())
            
            print(f"‚úÖ Procesados {total_logs} logs (bytes {bloque['byte_inicio']}-{bloque['byte_fin']})")
            
            return JSONResponse({
                "status": "success",
                "total_logs": total_logs,
                "bytes_processed": f"{bloque['byte_inicio']}-{bloque['byte_fin']}",
                "details": "Procesamiento completado (ver consola para detalles)"
            })
            
        except Exception as e:
            print(f"‚ùå Error procesando bloque: {str(e)}")
            ProcesosLogger.marcar_error(bloque['idAuditoria'])
            raise HTTPException(status_code=500, detail=str(e))
            
        finally:
            ProcesosLogger.finalizar_proceso(
                idAuditoria=bloque['idAuditoria'],
                totalLogs=total_logs if 'total_logs' in locals() else 0,
                ultimo_byte=bloque['byte_fin']
            )
            
    except Exception as e:
        print(f"‚ùå Error general: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))
        
    finally:
        if os.path.exists(temp_path):
            os.remove(temp_path)
            print(f"üßπ Archivo temporal eliminado: {temp_path}")

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
